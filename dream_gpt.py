# -*- coding: utf-8 -*-
"""dream gpt.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xXHWyEQgH2_WlWtwAcagEPTyttET8v9Z
"""

import pandas as pd
from transformers import GPT2Tokenizer

# Load the dataset
dataset_path = "/content/dream_data.csv"
df = pd.read_csv(dataset_path)

# Initialize the tokenizer with a padding token
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
tokenizer.add_special_tokens({'pad_token': '[PAD]'})

# Tokenize and preprocess the dream descriptions
max_length = 128  # Max length for padding/truncating
tokenized_dreams = []

for index, row in df.iterrows():
    # Concatenate dream description and dreamer comments for context
    context = row['Dream Description'] + " " + row['Dreamer Comments']

    # Tokenize and truncate/pad the combined context
    tokenized_context = tokenizer(context, max_length=max_length, truncation=True, padding='max_length')
    tokenized_dreams.append(tokenized_context)

# Convert tokenized sequences to tensors
dream_input_ids = [tokenized['input_ids'] for tokenized in tokenized_dreams]
dream_attention_masks = [tokenized['attention_mask'] for tokenized in tokenized_dreams]

# Optionally, you can convert the tokenized sequences to PyTorch tensors or TensorFlow tensors if you're using a deep learning framework

import torch
from transformers import GPT2LMHeadModel, GPT2Tokenizer, AdamW
from torch.utils.data import DataLoader, Dataset
from tqdm import tqdm

# Define the dataset class
class DreamDataset(Dataset):
    def __init__(self, input_ids, attention_masks):
        self.input_ids = input_ids
        self.attention_masks = attention_masks

    def __len__(self):
        return len(self.input_ids)

    def __getitem__(self, idx):
        return {
            "input_ids": torch.tensor(self.input_ids[idx]),
            "attention_mask": torch.tensor(self.attention_masks[idx]),
        }

# Initialize the GPT-2 model and tokenizer
model_name = "gpt2-medium"
tokenizer = GPT2Tokenizer.from_pretrained(model_name)
model = GPT2LMHeadModel.from_pretrained(model_name)

# Define the optimizer
optimizer = AdamW(model.parameters(), lr=5e-5)

# Define the dataset
dream_dataset = DreamDataset(dream_input_ids, dream_attention_masks)

# Define the data loader
train_loader = DataLoader(dream_dataset, batch_size=4, shuffle=True)

# Set the device (CPU or GPU)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

"""# Set the device (CPU or GPU)
import torch

# Check if TPU is available
if 'COLAB_TPU_ADDR' in os.environ:
    import torch_xla.core.xla_model as xm
    device = xm.xla_device()
    print("TPU available. Using TPU.")
else:
    # Fallback to GPU if TPU is not available
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print("TPU not available. Using GPU if available, else CPU.")

# Move model to device
model.to(device)

"""

# Training loop
num_epochs = 3
for epoch in range(num_epochs):
    model.train()
    total_loss = 0

    for batch in tqdm(train_loader, desc=f"Epoch {epoch + 1}"):
        # Move batch to device
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)

        # Ensure that input_ids fall within the acceptable range
        input_ids = torch.clamp(input_ids, 0, tokenizer.vocab_size - 1)

        # Forward pass
        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=input_ids)
        loss = outputs.loss

        # Backward pass
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    # Print average loss for the epoch
    average_loss = total_loss / len(train_loader)
    print(f"Epoch {epoch + 1}: Average Loss: {average_loss}")

# Save the trained model
model.save_pretrained("./saved_model")

# Save the tokenizer associated with the model
tokenizer.save_pretrained("./saved_model")

from google.colab import files

# Create a zip file of the saved model directory
!zip -r saved_model.zip saved_model

# Download the zip file
files.download("saved_model.zip")

!tar -cvf model.tar saved_model

from google.colab import drive
drive.mount('/content/drive')

# Move the model.tar file to Google Drive
!mv model.tar /content/drive/My\ Drive/

"""FROM NEXT DAY CODE

"""

from transformers import GPT2LMHeadModel, GPT2Tokenizer
import torch

# Load the saved model and tokenizer
model = GPT2LMHeadModel.from_pretrained("./saved_model")
tokenizer = GPT2Tokenizer.from_pretrained("./saved_model")

# Set the device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# Set the model to evaluation mode
model.eval()

# Example dream description to use as a prompt
dream_description = "I found myself trapped in a never-ending maze of twisting corridors, each shadow hiding unseen horrors lurking within. Every step echoed with the whispers of malevolent entities, their icy touch sending shivers down my spine."

"""##first try"""

# Tokenize the dream description
tokenized_prompt = tokenizer(dream_description, return_tensors="pt")

# Move input tensors to the appropriate device
input_ids = tokenized_prompt["input_ids"].to(device)
attention_mask = tokenized_prompt["attention_mask"].to(device)

# Generate text using the model
output = model.generate(input_ids=input_ids,
                        attention_mask=attention_mask,
                        max_length=200,
                        num_return_sequences=1,
                        temperature=0.7,
                        pad_token_id=tokenizer.eos_token_id,
                        do_sample=True,  # Enable sampling
                        top_k=50,         # Adjust top-k sampling parameter
                        top_p=0.95)       # Adjust nucleus sampling parameter

# Decode the generated output
generated_text = tokenizer.decode(output[0], skip_special_tokens=True)

# Print the generated text
print("Generated Text:")
print(generated_text)

"""##ends with a complete sentence"""

# Generate text using the model
output = model.generate(input_ids=input_ids,
                        attention_mask=attention_mask,
                        max_length=300,  # Adjust max_length to allow for longer text
                        num_return_sequences=1,
                        temperature=0.7,
                        pad_token_id=tokenizer.eos_token_id,
                        do_sample=True,  # Enable sampling
                        top_k=50,         # Adjust top-k sampling parameter
                        top_p=0.95)       # Adjust nucleus sampling parameter

# Decode the generated output
generated_text = tokenizer.decode(output[0], skip_special_tokens=True)

# Ensure the generated text ends with a complete sentence
sentences = generated_text.split(".")
if len(sentences) > 1:
    generated_text = ".".join(sentences[:-1]) + "."

# Print the generated text
print("Generated Text:")
print(generated_text)

"""##user enters the dream description and length of generated text

"""

# Prompt the user to input the dream description
dream_description = input("Enter the dream description: ")

# Tokenize the dream description
tokenized_prompt = tokenizer(dream_description, return_tensors="pt")

# Move input tensors to the appropriate device
input_ids = tokenized_prompt["input_ids"].to(device)
attention_mask = tokenized_prompt["attention_mask"].to(device)

# Prompt the user to input the desired maximum length
max_length_input = input("Enter the maximum length for the generated text: ")

# Convert the input to an integer
max_length = int(max_length_input)

# Generate text using the model
output = model.generate(input_ids=input_ids,
                        attention_mask=attention_mask,
                        max_length=max_length,  # Use the user-specified max_length
                        num_return_sequences=1,
                        temperature=0.7,
                        pad_token_id=tokenizer.eos_token_id,
                        do_sample=True,  # Enable sampling
                        top_k=50,         # Adjust top-k sampling parameter
                        top_p=0.95)       # Adjust nucleus sampling parameter

# Decode the generated output
generated_text = tokenizer.decode(output[0], skip_special_tokens=True)

# Ensure the generated text ends with a complete sentence
sentences = generated_text.split(".")
if len(sentences) > 1:
    generated_text = ".".join(sentences[:-1]) + "."

# Print the generated text
print("Generated Text:")
print(generated_text)